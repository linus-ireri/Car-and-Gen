// ingest.js
import { PDFLoader } from "@langchain/community/document_loaders/fs/pdf";
import { CheerioWebBaseLoader } from "@langchain/community/document_loaders/web/cheerio";
import { RecursiveCharacterTextSplitter } from "@langchain/textsplitters";
import { HNSWLib } from "@langchain/community/vectorstores/hnswlib";
import { HuggingFaceTransformersEmbeddings } from "@langchain/community/embeddings/huggingface_transformers";
import fs from "fs";
import path from "path";

// ===============================
// üìÅ SOURCE FILES & CONFIG
// ===============================
const pdfPaths = [
  "./docs/ComputerMisuseandCybercrimesActNo5of2018.pdf",
  "./docs/Privatization Act (1) 2025.pdf",
  "./docs/BasicEducationActNo_14of2013.pdf",
  "./docs/Media Council Act - Act No. 46 of 2013.pdf",
  "./docs/TheComputerMisuseandCybercrime_Amendment_Bill_2024.pdf"
];

const urls = [
  "https://www.kra.go.ke/en/helping-taxpayers/faqs"
];

const VECTOR_STORE_PATH = "./vector_store";


// üß† LOADERS
async function loadPDFs() {
  console.log("üìÑ Loading PDFs...");
  const docs = [];

  for (const pdfPath of pdfPaths) {
    try {
      if (!fs.existsSync(pdfPath)) {
        console.warn(`‚ö†Ô∏è  Skipping missing file: ${pdfPath}`);
        continue;
      }

      console.log(`  Loading: ${path.basename(pdfPath)}`);
      const loader = new PDFLoader(pdfPath, {
        splitPages: true,
      });

      const pdfDocs = await loader.load();

      const actName = path.basename(pdfPath)
        .replace(/\.pdf$/i, "")
        .replace(/_/g, " ")
        .trim();

      pdfDocs.forEach(doc => {
        doc.metadata = {
          ...doc.metadata,
          source: actName,
          type: "pdf"
        };
      });

      docs.push(...pdfDocs);
      console.log(`  ‚úì Loaded ${pdfDocs.length} pages`);
    } catch (error) {
      console.error(`  ‚úó Error loading ${pdfPath}:`, error.message);
    }
  }

  return docs;
}

async function loadURLs() {
  console.log("\nüåê Loading URLs...");
  const docs = [];

  for (const url of urls) {
    try {
      console.log(`  Scraping: ${url}`);
      const loader = new CheerioWebBaseLoader(url);
      const urlDocs = await loader.load();

      urlDocs.forEach(doc => {
        doc.metadata = {
          ...doc.metadata,
          source: url,
          type: "web"
        };
      });

      docs.push(...urlDocs);
      console.log(`  ‚úì Loaded ${urlDocs.length} documents`);
    } catch (error) {
      console.error(`  ‚úó Error loading ${url}:`, error.message);
    }
  }

  return docs;
}

// ===============================
// üß© MAIN INGESTION PIPELINE
// ===============================
async function main() {
  console.log("üöÄ Starting RAG ingestion pipeline...\n");

  // 1. Load all sources
  const pdfDocs = await loadPDFs();
  const urlDocs = await loadURLs();
  const allDocs = [...pdfDocs, ...urlDocs];

  if (allDocs.length === 0) {
    console.error("‚ùå No documents loaded. Exiting.");
    process.exit(1);
  }

  console.log(`\nüìä Total documents loaded: ${allDocs.length}`);

  // 2. Smart splitting for legal documents
  console.log("\n‚úÇÔ∏è  Splitting documents into structured chunks...");
  const splitter = new RecursiveCharacterTextSplitter({
    chunkSize: 700,
    chunkOverlap: 100,
    separators: ["\nSection ", "\nPART ", "\nCHAPTER ", "\n\n", ". "]
  });

  const splitDocs = await splitter.splitDocuments(allDocs);
  console.log(`  ‚úì Created ${splitDocs.length} section-based chunks`);

  // 3. Clean and normalize
  const cleanDocs = splitDocs.map(doc => {
    doc.pageContent = doc.pageContent
      .replace(/Page\s\d+\sof\s\d+/gi, "")
      .replace(/\s{2,}/g, " ")
      .trim();
    return doc;
  });

  // 4. Initialize embeddings model (CPU-optimized)
  console.log("\nü§ñ Loading embedding model...");
  const embeddings = new HuggingFaceTransformersEmbeddings({
    modelName: "Xenova/all-MiniLM-L6-v2",
    batchSize: 8 // ‚úÖ small batch size prevents memory errors
  });

  // 5. Create vector store incrementally
  console.log("\nüî¢ Creating vector store in safe batches...");
  if (!fs.existsSync(VECTOR_STORE_PATH)) {
    fs.mkdirSync(VECTOR_STORE_PATH, { recursive: true });
  }

  const vectorStore = await HNSWLib.fromDocuments([], embeddings);

  const BATCH_SIZE = 50;
  for (let i = 0; i < cleanDocs.length; i += BATCH_SIZE) {
    const batch = cleanDocs.slice(i, i + BATCH_SIZE);
    console.log(`  ‚Üí Processing batch ${Math.ceil(i / BATCH_SIZE) + 1}/${Math.ceil(cleanDocs.length / BATCH_SIZE)}...`);
    await vectorStore.addDocuments(batch);
    global.gc?.(); // Force garbage collection if available
  }

  await vectorStore.save(VECTOR_STORE_PATH);
  console.log(`\nüíæ Vector store saved to: ${VECTOR_STORE_PATH}`);

  // 6. Summary
  console.log("\n‚úÖ Ingestion complete!");
  console.log("üìà Summary:");
  console.log(`  - PDFs processed: ${pdfPaths.length}`);
  console.log(`  - URLs scraped: ${urls.length}`);
  console.log(`  - Total chunks: ${cleanDocs.length}`);
  console.log(`  - Vector store location: ${VECTOR_STORE_PATH}`);
}

main().catch(err => {
  console.error("\n‚ùå Ingestion failed:");
  console.error(err);
  process.exit(1);
});
